<<<<<<< Updated upstream
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autodiff.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:34.181987Z",
          "iopub.status.busy": "2021-03-19T01:22:34.181398Z",
          "iopub.status.idle": "2021-03-19T01:22:34.183269Z",
          "shell.execute_reply": "2021-03-19T01:22:34.183646Z"
        },
        "id": "tuOe1ymfHZPu"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introduction to gradients and automatic differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6P32iYYV27b"
      },
      "source": [
        "## Automatic Differentiation and Gradients\n",
        "\n",
        "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
        "is useful for implementing machine learning algorithms such as\n",
        "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for training\n",
        "neural networks.\n",
        "\n",
        "In this guide, you will explore ways to compute gradients with TensorFlow, especially in [eager execution](eager.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:34.191632Z",
          "iopub.status.busy": "2021-03-19T01:22:34.191017Z",
          "iopub.status.idle": "2021-03-19T01:22:41.085639Z",
          "shell.execute_reply": "2021-03-19T01:22:41.085098Z"
        },
        "id": "IqR2PQG4ZaZ0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Computing gradients\n",
        "\n",
        "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the *forward* pass.  Then, during the *backward pass*, TensorFlow traverses this list of operations in reverse order to compute gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "## Gradient tapes\n",
        "\n",
        "TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s.\n",
        "TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
        "\n",
        "Here is a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:41.090025Z",
          "iopub.status.busy": "2021-03-19T01:22:41.089318Z",
          "iopub.status.idle": "2021-03-19T01:22:50.217391Z",
          "shell.execute_reply": "2021-03-19T01:22:50.216802Z"
        },
        "id": "Xq9GgTCP7a4A"
      },
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR9tFAP_7cra"
      },
      "source": [
        "Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.225986Z",
          "iopub.status.busy": "2021-03-19T01:22:50.225380Z",
          "iopub.status.idle": "2021-03-19T01:22:50.234791Z",
          "shell.execute_reply": "2021-03-19T01:22:50.235183Z"
        },
        "id": "LsvrwF6bHroC",
        "outputId": "527a3165-f181-43e9-d99e-98ed887c48e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2_aqsO25Vx1"
      },
      "source": [
        "The above example uses scalars, but `tf.GradientTape` works as easily on any tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.241951Z",
          "iopub.status.busy": "2021-03-19T01:22:50.241342Z",
          "iopub.status.idle": "2021-03-19T01:22:50.632250Z",
          "shell.execute_reply": "2021-03-19T01:22:50.631662Z"
        },
        "id": "vacZ3-Ws5VdV"
      },
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eXOkrQ-9Pb"
      },
      "source": [
        "To get the gradient of `y` with respect to both variables, you can pass both as sources to the `gradient` method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.636966Z",
          "iopub.status.busy": "2021-03-19T01:22:50.636337Z",
          "iopub.status.idle": "2021-03-19T01:22:50.642485Z",
          "shell.execute_reply": "2021-03-19T01:22:50.642025Z"
        },
        "id": "luOtK1Da_BR0",
        "outputId": "32213428-7ff3-4a9a-e750-a27debe8d1e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b]) # dl_dw is \"dw\" in Andrew's course\n",
        "dl_db.numpy()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-7.387056,  7.342686], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei4iVXi6qgM7"
      },
      "source": [
        "The gradient with respect to each source has the shape of the source:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.646836Z",
          "iopub.status.busy": "2021-03-19T01:22:50.645901Z",
          "iopub.status.idle": "2021-03-19T01:22:50.648937Z",
          "shell.execute_reply": "2021-03-19T01:22:50.649350Z"
        },
        "id": "aYbWRFPZqk4U",
        "outputId": "0408ce54-5759-476c-c12e-3ebda8b23352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2)\n",
            "(3, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI_SzxHsvao1"
      },
      "source": [
        "Here is the gradient calculation again, this time passing a dictionary of variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.653735Z",
          "iopub.status.busy": "2021-03-19T01:22:50.653134Z",
          "iopub.status.idle": "2021-03-19T01:22:50.658299Z",
          "shell.execute_reply": "2021-03-19T01:22:50.657830Z"
        },
        "id": "d73cY6NOuaMd",
        "outputId": "9fdac68b-c920-4573-b583-40ffb68de47f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "my_vars = {\n",
        "    'w': w,\n",
        "    'b': b\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b'].numpy()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-7.387056,  7.342686], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ2LvHifEMgO"
      },
      "source": [
        "## Gradients with respect to a model\n",
        "\n",
        "It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer`, `keras.Model`) for [checkpointing](checkpoint.ipynb) and [exporting](saved_model.ipynb).\n",
        "\n",
        "In most cases, you will want to calculate gradients with respect to a model's trainable variables.  Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.663214Z",
          "iopub.status.busy": "2021-03-19T01:22:50.662558Z",
          "iopub.status.idle": "2021-03-19T01:22:50.682905Z",
          "shell.execute_reply": "2021-03-19T01:22:50.682350Z"
        },
        "id": "JvesHtbQESc-"
      },
      "source": [
        "layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Forward pass\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.687173Z",
          "iopub.status.busy": "2021-03-19T01:22:50.686433Z",
          "iopub.status.idle": "2021-03-19T01:22:50.689459Z",
          "shell.execute_reply": "2021-03-19T01:22:50.689010Z"
        },
        "id": "PR_ezr6UFrpI",
        "outputId": "b82f559f-2b62-4e87-9aa5-ea9d86b3c5dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f\"var_name: '{var.name}', shape: {g.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "var_name: 'dense/kernel:0', shape: (3, 2)\n",
            "var_name: 'dense/bias:0', shape: (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Gx6LS714zR"
      },
      "source": [
        "<a id=\"watches\"></a>\n",
        "\n",
        "## Controlling what the tape watches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4VlqKFzzGaC"
      },
      "source": [
        "The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n",
        "\n",
        "* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
        "* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
        "* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
        "\n",
        "For example, the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.695152Z",
          "iopub.status.busy": "2021-03-19T01:22:50.694566Z",
          "iopub.status.idle": "2021-03-19T01:22:50.699910Z",
          "shell.execute_reply": "2021-03-19T01:22:50.699359Z"
        },
        "id": "Kj9gPckdB37a",
        "outputId": "c9827002-82bd-4e37-d424-a59b7ace3480"
      },
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "# Not a variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkcpQnLgNxgi"
      },
      "source": [
        "You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.704452Z",
          "iopub.status.busy": "2021-03-19T01:22:50.703697Z",
          "iopub.status.idle": "2021-03-19T01:22:50.706488Z",
          "shell.execute_reply": "2021-03-19T01:22:50.706887Z"
        },
        "id": "hwNwjW1eAkib",
        "outputId": "137e41f9-ced3-4005-d342-87b80d07fdd6"
      },
      "source": [
        "[var.name for var in tape.watched_variables()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['x0:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB9I1uFvB4tf"
      },
      "source": [
        "`tf.GradientTape` provides hooks that give the user control over what is or is not watched.\n",
        "\n",
        "To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.711808Z",
          "iopub.status.busy": "2021-03-19T01:22:50.710756Z",
          "iopub.status.idle": "2021-03-19T01:22:50.714390Z",
          "shell.execute_reply": "2021-03-19T01:22:50.713973Z"
        },
        "id": "tVN1QqFRDHBK",
        "outputId": "ae026328-c60e-4565-d356-225f3f511249"
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxsiYnf2DN8K"
      },
      "source": [
        "Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.718837Z",
          "iopub.status.busy": "2021-03-19T01:22:50.718258Z",
          "iopub.status.idle": "2021-03-19T01:22:50.722142Z",
          "shell.execute_reply": "2021-03-19T01:22:50.721679Z"
        },
        "id": "7QPzwWvSEwIp"
      },
      "source": [
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x1)\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRduLbE1H2IJ"
      },
      "source": [
        "Since `GradientTape.watch` was not called on `x0`, no gradient is computed with respect to it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.726408Z",
          "iopub.status.busy": "2021-03-19T01:22:50.725517Z",
          "iopub.status.idle": "2021-03-19T01:22:50.729769Z",
          "shell.execute_reply": "2021-03-19T01:22:50.729247Z"
        },
        "id": "e6GM-3evH1Sz",
        "outputId": "81304664-1319-4601-b626-d70b47cad521"
      },
      "source": [
        "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "\n",
        "print('dy/dx0:', grad['x0'])\n",
        "print('dy/dx1:', grad['x1'].numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dy/dx0: None\n",
            "dy/dx1: 0.9999546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g1nKB6P-OnA"
      },
      "source": [
        "## Intermediate results\n",
        "\n",
        "You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.734771Z",
          "iopub.status.busy": "2021-03-19T01:22:50.733754Z",
          "iopub.status.idle": "2021-03-19T01:22:50.737344Z",
          "shell.execute_reply": "2021-03-19T01:22:50.736899Z"
        },
        "id": "7XaPRAwUyYms",
        "outputId": "ed51f883-3433-4d6b-8af5-dc1d2f1da588"
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "# Use the tape to compute the gradient of z with respect to the\n",
        "# intermediate value y.\n",
        "# dz_dx = 2 * y, where y = x ** 2 (= 9)\n",
        "print(tape.gradient(z, y).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISkXuY7YzIcS"
      },
      "source": [
        "By default, the resources held by a `GradientTape` are released as soon as the `GradientTape.gradient` method is called. To compute multiple gradients over the same computation, create a gradient tape with `persistent=True`. This allows multiple calls to the `gradient` method as resources are released when the tape object is garbage collected. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.741719Z",
          "iopub.status.busy": "2021-03-19T01:22:50.741071Z",
          "iopub.status.idle": "2021-03-19T01:22:50.745659Z",
          "shell.execute_reply": "2021-03-19T01:22:50.746022Z"
        },
        "id": "zZaCm3-9zVCi",
        "outputId": "26157147-975c-4377-c629-7004bde301ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "print(tape.gradient(z, y).numpy())  # 18.0 (2 * x**2)\n",
        "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18.0\n",
            "6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.749607Z",
          "iopub.status.busy": "2021-03-19T01:22:50.748988Z",
          "iopub.status.idle": "2021-03-19T01:22:50.751260Z",
          "shell.execute_reply": "2021-03-19T01:22:50.751680Z"
        },
        "id": "j8bv_jQFg6CN"
      },
      "source": [
        "del tape   # Drop the reference to the tape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ZY-9BUB7vX"
      },
      "source": [
        "## Notes on performance\n",
        "\n",
        "* There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n",
        "\n",
        "* Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n",
        "\n",
        "  For efficiency, some ops (like `ReLU`) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use `persistent=True` on your tape, *nothing is discarded* and your peak memory usage will be higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dLBpZsJebFq"
      },
      "source": [
        "## Gradients of non-scalar targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pldU9F5duP2"
      },
      "source": [
        "A gradient is fundamentally an operation on a scalar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.756995Z",
          "iopub.status.busy": "2021-03-19T01:22:50.756351Z",
          "iopub.status.idle": "2021-03-19T01:22:50.761446Z",
          "shell.execute_reply": "2021-03-19T01:22:50.760975Z"
        },
        "id": "qI0sDV_WeXBb",
        "outputId": "49fb542b-077d-42c1-87b6-65e41ae75505"
      },
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient(y0, x).numpy())\n",
        "print(tape.gradient(y1, x).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n",
            "-0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COEyYp34fxj4"
      },
      "source": [
        "Thus, if you ask for the gradient of multiple targets, the result for each source is:\n",
        "\n",
        "* The gradient of the sum of the targets, or equivalently\n",
        "* The sum of the gradients of each target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.765972Z",
          "iopub.status.busy": "2021-03-19T01:22:50.765297Z",
          "iopub.status.idle": "2021-03-19T01:22:50.769183Z",
          "shell.execute_reply": "2021-03-19T01:22:50.769576Z"
        },
        "id": "o4a6_YOcfWKS",
        "outputId": "8f4e9ae2-689e-4066-ca81-015e235b558d"
      },
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvP-mkBMgbym"
      },
      "source": [
        "Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.774011Z",
          "iopub.status.busy": "2021-03-19T01:22:50.773407Z",
          "iopub.status.idle": "2021-03-19T01:22:50.777405Z",
          "shell.execute_reply": "2021-03-19T01:22:50.777803Z"
        },
        "id": "DArPWqsSh5un",
        "outputId": "a6f9e552-8d28-4552-a85e-5aedfd8cc306"
      },
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * [3., 4.]\n",
        "\n",
        "print(tape.gradient(y, x).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flDbx68Zh5Lb"
      },
      "source": [
        "This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n",
        "\n",
        "If you need a separate gradient for each item, refer to [Jacobians](advanced_autodiff.ipynb#jacobians)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwFswok8RAly"
      },
      "source": [
        "In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.782532Z",
          "iopub.status.busy": "2021-03-19T01:22:50.781949Z",
          "iopub.status.idle": "2021-03-19T01:22:50.790536Z",
          "shell.execute_reply": "2021-03-19T01:22:50.790983Z"
        },
        "id": "JQvk_jnMmTDS"
      },
      "source": [
        "x = tf.linspace(-10.0, 10.0, 200+1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.nn.sigmoid(x)\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:50.810395Z",
          "iopub.status.busy": "2021-03-19T01:22:50.803457Z",
          "iopub.status.idle": "2021-03-19T01:22:51.055330Z",
          "shell.execute_reply": "2021-03-19T01:22:51.055955Z"
        },
        "id": "e_f2QgDPmcPE",
        "outputId": "3be4e00c-ecd9-4059-dc09-d8aae752b93b"
      },
      "source": [
        "plt.plot(x, y, label='y')\n",
        "plt.plot(x, dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kADybtQzYj4"
      },
      "source": [
        "## Control flow\n",
        "\n",
        "Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, `if` and `while` statements).\n",
        "\n",
        "Here a different variable is used on each branch of an `if`. The gradient only connects to the variable that was used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.062698Z",
          "iopub.status.busy": "2021-03-19T01:22:51.062006Z",
          "iopub.status.idle": "2021-03-19T01:22:51.066812Z",
          "shell.execute_reply": "2021-03-19T01:22:51.066299Z"
        },
        "id": "ciFLizhrrjy7",
        "outputId": "f7d3cfd7-a4f8-4586-fe25-b43eff8f0b92"
      },
      "source": [
        "x = tf.constant(1.0)\n",
        "\n",
        "v0 = tf.Variable(2.0)\n",
        "v1 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  if x > 0.0:\n",
        "    result = v0\n",
        "  else:\n",
        "    result = v1**2 \n",
        "\n",
        "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
        "\n",
        "print(dv0)\n",
        "print(dv1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKnLaiapsjeP"
      },
      "source": [
        "Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n",
        "\n",
        "Depending on the value of `x` in the above example, the tape either records `result = v0` or `result = v1**2`. The gradient with respect to `x` is always `None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.071362Z",
          "iopub.status.busy": "2021-03-19T01:22:51.070643Z",
          "iopub.status.idle": "2021-03-19T01:22:51.072987Z",
          "shell.execute_reply": "2021-03-19T01:22:51.073393Z"
        },
        "id": "8k05WmuAwPm7",
        "outputId": "f69bae31-504d-456c-fc10-69864a9b6a9d"
      },
      "source": [
        "dx = tape.gradient(result, x)\n",
        "\n",
        "print(dx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egypBxISAHhx"
      },
      "source": [
        "## Getting a gradient of `None`\n",
        "\n",
        "When a target is not connected to a source you will get a gradient of `None`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.077800Z",
          "iopub.status.busy": "2021-03-19T01:22:51.077181Z",
          "iopub.status.idle": "2021-03-19T01:22:51.081328Z",
          "shell.execute_reply": "2021-03-19T01:22:51.080841Z"
        },
        "id": "CU185WDM81Ut",
        "outputId": "2ab6fa58-e567-4d69-d733-b75edec817c5"
      },
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y * y\n",
        "print(tape.gradient(z, x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZbKpHfBRJym"
      },
      "source": [
        "Here `z` is obviously not connected to `x`, but there are several less-obvious ways that a gradient can be disconnected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHDzDOiQ8xmw"
      },
      "source": [
        "### 1. Replaced a variable with a tensor\n",
        "\n",
        "In the section on [\"controlling what the tape watches\"](#watches) you saw that the tape will automatically watch a `tf.Variable` but not a `tf.Tensor`.\n",
        "\n",
        "One common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.086395Z",
          "iopub.status.busy": "2021-03-19T01:22:51.085557Z",
          "iopub.status.idle": "2021-03-19T01:22:51.090278Z",
          "shell.execute_reply": "2021-03-19T01:22:51.089801Z"
        },
        "id": "QPKY4Tn9zX7_",
        "outputId": "fc3373e3-3da4-472a-8d76-d02287bf74d2"
      },
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "for epoch in range(2):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = x+1\n",
        "\n",
        "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
        "  x = x + 1   # This should be `x.assign_add(1)`"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "EagerTensor : None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gwZKxgA97an"
      },
      "source": [
        "### 2. Did calculations outside of TensorFlow\n",
        "\n",
        "The tape can't record the gradient path if the calculation exits TensorFlow.\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.095376Z",
          "iopub.status.busy": "2021-03-19T01:22:51.094577Z",
          "iopub.status.idle": "2021-03-19T01:22:51.098167Z",
          "shell.execute_reply": "2021-03-19T01:22:51.098504Z"
        },
        "id": "jmoLCDJb_yw1",
        "outputId": "f04f2286-db78-4e6a-dc08-0d2e4e48c74a"
      },
      "source": [
        "x = tf.Variable([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  x2 = x**2\n",
        "\n",
        "  # This step is calculated with NumPy\n",
        "  y = np.mean(x2, axis=0)\n",
        "\n",
        "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
        "  # using `tf.convert_to_tensor`.\n",
        "  y = tf.reduce_mean(y, axis=0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3YVfP3R-tp7"
      },
      "source": [
        "### 3. Took gradients through an integer or string\n",
        "\n",
        "Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n",
        "\n",
        "Nobody expects strings to be differentiable, but it's easy to accidentally create an `int` constant or variable if you don't specify the `dtype`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.103946Z",
          "iopub.status.busy": "2021-03-19T01:22:51.103117Z",
          "iopub.status.idle": "2021-03-19T01:22:51.108993Z",
          "shell.execute_reply": "2021-03-19T01:22:51.108487Z"
        },
        "id": "9jlHXHqfASU3",
        "outputId": "a3f117fc-1eed-4188-986c-137dad144988"
      },
      "source": [
        "x = tf.constant(10)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "  g.watch(x)\n",
        "  y = x * x\n",
        "\n",
        "print(g.gradient(y, x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsdP_mTHX9L1"
      },
      "source": [
        "TensorFlow doesn't automatically cast between types, so, in practice, you'll often get a type error instead of a missing gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyAZ7C8qCEs6"
      },
      "source": [
        "### 4. Took gradients through a stateful object\n",
        "\n",
        "State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n",
        "\n",
        "A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a _value_, but no _state_. All the operations discussed so far are also stateless: the output of a `tf.matmul` only depends on its inputs.\n",
        "\n",
        "A `tf.Variable` has internal state—its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.113936Z",
          "iopub.status.busy": "2021-03-19T01:22:51.113311Z",
          "iopub.status.idle": "2021-03-19T01:22:51.117358Z",
          "shell.execute_reply": "2021-03-19T01:22:51.117693Z"
        },
        "id": "C1tLeeRFE479",
        "outputId": "cb32c433-2258-4f94-ca62-a0019c9ba045"
      },
      "source": [
        "x0 = tf.Variable(3.0)\n",
        "x1 = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Update x1 = x1 + x0.\n",
        "  x1.assign_add(x0)\n",
        "  # The tape starts recording from x1.\n",
        "  y = x1**2   # y = (x1 + x0)**2\n",
        "\n",
        "# This doesn't work.\n",
        "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKA92-dqF2r-"
      },
      "source": [
        "Similarly, `tf.data.Dataset` iterators and `tf.queue`s are stateful, and will stop all gradients on tensors that pass through them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHvcDGIbOj2I"
      },
      "source": [
        "## No gradient registered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoc-A6AxVqry"
      },
      "source": [
        "Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n",
        "\n",
        "The `tf.raw_ops` page shows which low-level ops have gradients registered.\n",
        "\n",
        "If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning `None`. This way you know something has gone wrong.\n",
        "\n",
        "For example, the `tf.image.adjust_contrast` function wraps `raw_ops.AdjustContrastv2`, which could have a gradient but the gradient is not implemented:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.122994Z",
          "iopub.status.busy": "2021-03-19T01:22:51.122410Z",
          "iopub.status.idle": "2021-03-19T01:22:51.126621Z",
          "shell.execute_reply": "2021-03-19T01:22:51.127028Z"
        },
        "id": "HSb20FXc_V0U",
        "outputId": "54795094-6d4c-45c8-88dd-b9a20f6c09b8"
      },
      "source": [
        "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
        "delta = tf.Variable(0.1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  new_image = tf.image.adjust_contrast(image, delta)\n",
        "\n",
        "try:\n",
        "  print(tape.gradient(new_image, [image, delta]))\n",
        "  assert False   # This should not happen.\n",
        "except LookupError as e:\n",
        "  print(f'{type(e).__name__}: {e}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LookupError: gradient registry has no entry for: AdjustContrastv2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDoutjzATiEm"
      },
      "source": [
        "If you need to differentiate through this op, you'll either need to implement the gradient and register it (using `tf.RegisterGradient`) or re-implement the function using other ops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCTwc_dQXp2W"
      },
      "source": [
        "## Zeros instead of None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYDrVogA89eA"
      },
      "source": [
        "In some cases it would be convenient to get 0 instead of `None` for unconnected gradients.  You can decide what to return when you have unconnected gradients using the `unconnected_gradients` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-19T01:22:51.131767Z",
          "iopub.status.busy": "2021-03-19T01:22:51.131053Z",
          "iopub.status.idle": "2021-03-19T01:22:51.135770Z",
          "shell.execute_reply": "2021-03-19T01:22:51.135265Z"
        },
        "id": "U6zxk1sf9Ixx",
        "outputId": "2e807d8a-95b4-421f-ee11-a76b7a35e129"
      },
      "source": [
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y**2\n",
        "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
=======
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of autodiff.ipynb","provenance":[{"file_id":"https://github.com/blacksnail789521/Programming_Practice/blob/master/ML/TensorFlow_guide/TensorFlow%20basics/04_Introduction%20to%20gradients%20and%20automatic%20differentiation.ipynb","timestamp":1616488153013}],"collapsed_sections":["Tce3stUlHN0L"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Tce3stUlHN0L"},"source":["##### Copyright 2020 The TensorFlow Authors."]},{"cell_type":"code","metadata":{"cellView":"form","execution":{"iopub.execute_input":"2021-03-19T01:22:34.181987Z","iopub.status.busy":"2021-03-19T01:22:34.181398Z","iopub.status.idle":"2021-03-19T01:22:34.183269Z","shell.execute_reply":"2021-03-19T01:22:34.183646Z"},"id":"tuOe1ymfHZPu","executionInfo":{"status":"ok","timestamp":1616487895051,"user_tz":-480,"elapsed":1420,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFdPvlXBOdUN"},"source":["# Introduction to gradients and automatic differentiation"]},{"cell_type":"markdown","metadata":{"id":"MfBg1C5NB3X0"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"r6P32iYYV27b"},"source":["## Automatic Differentiation and Gradients\n","\n","[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n","is useful for implementing machine learning algorithms such as\n","[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for training\n","neural networks.\n","\n","In this guide, you will explore ways to compute gradients with TensorFlow, especially in [eager execution](eager.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"MUXex9ctTuDB"},"source":["## Setup"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:34.191632Z","iopub.status.busy":"2021-03-19T01:22:34.191017Z","iopub.status.idle":"2021-03-19T01:22:41.085639Z","shell.execute_reply":"2021-03-19T01:22:41.085098Z"},"id":"IqR2PQG4ZaZ0","executionInfo":{"status":"ok","timestamp":1616487897669,"user_tz":-480,"elapsed":4022,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xHxb-dlhMIzW"},"source":["## Computing gradients\n","\n","To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the *forward* pass.  Then, during the *backward pass*, TensorFlow traverses this list of operations in reverse order to compute gradients."]},{"cell_type":"markdown","metadata":{"id":"1CLWJl0QliB0"},"source":["## Gradient tapes\n","\n","TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s.\n","TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n","\n","Here is a simple example:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:41.090025Z","iopub.status.busy":"2021-03-19T01:22:41.089318Z","iopub.status.idle":"2021-03-19T01:22:50.217391Z","shell.execute_reply":"2021-03-19T01:22:50.216802Z"},"id":"Xq9GgTCP7a4A","executionInfo":{"status":"ok","timestamp":1616487902846,"user_tz":-480,"elapsed":9186,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["x = tf.Variable(3.0)\n","\n","with tf.GradientTape() as tape:\n","  y = x**2"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CR9tFAP_7cra"},"source":["Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.225986Z","iopub.status.busy":"2021-03-19T01:22:50.225380Z","iopub.status.idle":"2021-03-19T01:22:50.234791Z","shell.execute_reply":"2021-03-19T01:22:50.235183Z"},"id":"LsvrwF6bHroC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487902849,"user_tz":-480,"elapsed":9173,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"8a65ebf3-d4b2-466d-d689-7645b28d1c65"},"source":["# dy = 2x * dx\n","dy_dx = tape.gradient(y, x)\n","dy_dx.numpy()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.0"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Q2_aqsO25Vx1"},"source":["The above example uses scalars, but `tf.GradientTape` works as easily on any tensor:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.241951Z","iopub.status.busy":"2021-03-19T01:22:50.241342Z","iopub.status.idle":"2021-03-19T01:22:50.632250Z","shell.execute_reply":"2021-03-19T01:22:50.631662Z"},"id":"vacZ3-Ws5VdV","executionInfo":{"status":"ok","timestamp":1616487904977,"user_tz":-480,"elapsed":11283,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["w = tf.Variable(tf.random.normal((3, 2)), name='w')\n","b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n","x = [[1., 2., 3.]]\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  y = x @ w + b\n","  loss = tf.reduce_mean(y**2)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4eXOkrQ-9Pb"},"source":["To get the gradient of `y` with respect to both variables, you can pass both as sources to the `gradient` method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.636966Z","iopub.status.busy":"2021-03-19T01:22:50.636337Z","iopub.status.idle":"2021-03-19T01:22:50.642485Z","shell.execute_reply":"2021-03-19T01:22:50.642025Z"},"id":"luOtK1Da_BR0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904979,"user_tz":-480,"elapsed":11273,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"4ff69fc5-4d3e-4ee1-936f-dbc86373c911"},"source":["[dl_dw, dl_db] = tape.gradient(loss, [w, b]) # dl_dw is \"dw\" in Andrew's course\n","dl_db.numpy()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-2.4369016,  3.9588737], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Ei4iVXi6qgM7"},"source":["The gradient with respect to each source has the shape of the source:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.646836Z","iopub.status.busy":"2021-03-19T01:22:50.645901Z","iopub.status.idle":"2021-03-19T01:22:50.648937Z","shell.execute_reply":"2021-03-19T01:22:50.649350Z"},"id":"aYbWRFPZqk4U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904980,"user_tz":-480,"elapsed":11260,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"bdbe8540-ecf9-46d2-86ba-21cad9d801dc"},"source":["print(w.shape)\n","print(dl_dw.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(3, 2)\n","(3, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dI_SzxHsvao1"},"source":["Here is the gradient calculation again, this time passing a dictionary of variables:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.653735Z","iopub.status.busy":"2021-03-19T01:22:50.653134Z","iopub.status.idle":"2021-03-19T01:22:50.658299Z","shell.execute_reply":"2021-03-19T01:22:50.657830Z"},"id":"d73cY6NOuaMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904981,"user_tz":-480,"elapsed":11247,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"655205df-944c-409c-c87c-b1c482690cf9"},"source":["my_vars = {\n","    'w': w,\n","    'b': b\n","}\n","\n","grad = tape.gradient(loss, my_vars)\n","grad['b'].numpy()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-2.4369016,  3.9588737], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"HZ2LvHifEMgO"},"source":["## Gradients with respect to a model\n","\n","It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer`, `keras.Model`) for [checkpointing](checkpoint.ipynb) and [exporting](saved_model.ipynb).\n","\n","In most cases, you will want to calculate gradients with respect to a model's trainable variables.  Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.663214Z","iopub.status.busy":"2021-03-19T01:22:50.662558Z","iopub.status.idle":"2021-03-19T01:22:50.682905Z","shell.execute_reply":"2021-03-19T01:22:50.682350Z"},"id":"JvesHtbQESc-","executionInfo":{"status":"ok","timestamp":1616487904982,"user_tz":-480,"elapsed":11235,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["layer = tf.keras.layers.Dense(2, activation='relu')\n","x = tf.constant([[1., 2., 3.]])\n","\n","with tf.GradientTape() as tape:\n","  # Forward pass\n","  y = layer(x)\n","  loss = tf.reduce_mean(y**2)\n","\n","# Calculate gradients with respect to every trainable variable\n","grad = tape.gradient(loss, layer.trainable_variables)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.687173Z","iopub.status.busy":"2021-03-19T01:22:50.686433Z","iopub.status.idle":"2021-03-19T01:22:50.689459Z","shell.execute_reply":"2021-03-19T01:22:50.689010Z"},"id":"PR_ezr6UFrpI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904983,"user_tz":-480,"elapsed":11225,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"b0765c1a-734d-44e0-d543-2859591e506f"},"source":["for var, g in zip(layer.trainable_variables, grad):\n","  print(f\"var_name: '{var.name}', shape: {g.shape}\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["var_name: 'dense/kernel:0', shape: (3, 2)\n","var_name: 'dense/bias:0', shape: (2,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f6Gx6LS714zR"},"source":["<a id=\"watches\"></a>\n","\n","## Controlling what the tape watches"]},{"cell_type":"markdown","metadata":{"id":"N4VlqKFzzGaC"},"source":["The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n","\n","* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n","* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n","* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n","\n","For example, the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.695152Z","iopub.status.busy":"2021-03-19T01:22:50.694566Z","iopub.status.idle":"2021-03-19T01:22:50.699910Z","shell.execute_reply":"2021-03-19T01:22:50.699359Z"},"id":"Kj9gPckdB37a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904984,"user_tz":-480,"elapsed":11211,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"4838334c-bda4-433d-a2d9-cac24c7d1b78"},"source":["# A trainable variable\n","x0 = tf.Variable(3.0, name='x0')\n","# Not trainable\n","x1 = tf.Variable(3.0, name='x1', trainable=False)\n","# Not a Variable: A variable + tensor returns a tensor.\n","x2 = tf.Variable(2.0, name='x2') + 1.0\n","# Not a variable\n","x3 = tf.constant(3.0, name='x3')\n","\n","with tf.GradientTape() as tape:\n","  y = (x0**2) + (x1**2) + (x2**2)\n","\n","grad = tape.gradient(y, [x0, x1, x2, x3])\n","\n","for g in grad:\n","  print(g)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tf.Tensor(6.0, shape=(), dtype=float32)\n","None\n","None\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RkcpQnLgNxgi"},"source":["You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.704452Z","iopub.status.busy":"2021-03-19T01:22:50.703697Z","iopub.status.idle":"2021-03-19T01:22:50.706488Z","shell.execute_reply":"2021-03-19T01:22:50.706887Z"},"id":"hwNwjW1eAkib","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904985,"user_tz":-480,"elapsed":11196,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"8a446ffc-e1ae-4f8b-b9b0-eb89d6897fb0"},"source":["[var.name for var in tape.watched_variables()]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['x0:0']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"NB9I1uFvB4tf"},"source":["`tf.GradientTape` provides hooks that give the user control over what is or is not watched.\n","\n","To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.711808Z","iopub.status.busy":"2021-03-19T01:22:50.710756Z","iopub.status.idle":"2021-03-19T01:22:50.714390Z","shell.execute_reply":"2021-03-19T01:22:50.713973Z"},"id":"tVN1QqFRDHBK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904986,"user_tz":-480,"elapsed":11181,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"edf70e07-0ae9-481d-dff9-b5398e58a407"},"source":["x = tf.constant(3.0)\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x**2\n","\n","# dy = 2x * dx\n","dy_dx = tape.gradient(y, x)\n","print(dy_dx.numpy())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["6.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qxsiYnf2DN8K"},"source":["Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.718837Z","iopub.status.busy":"2021-03-19T01:22:50.718258Z","iopub.status.idle":"2021-03-19T01:22:50.722142Z","shell.execute_reply":"2021-03-19T01:22:50.721679Z"},"id":"7QPzwWvSEwIp","executionInfo":{"status":"ok","timestamp":1616487904987,"user_tz":-480,"elapsed":11170,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["x0 = tf.Variable(0.0)\n","x1 = tf.Variable(10.0)\n","\n","with tf.GradientTape(watch_accessed_variables=False) as tape:\n","  tape.watch(x1)\n","  y0 = tf.math.sin(x0)\n","  y1 = tf.nn.softplus(x1)\n","  y = y0 + y1\n","  ys = tf.reduce_sum(y)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TRduLbE1H2IJ"},"source":["Since `GradientTape.watch` was not called on `x0`, no gradient is computed with respect to it:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.726408Z","iopub.status.busy":"2021-03-19T01:22:50.725517Z","iopub.status.idle":"2021-03-19T01:22:50.729769Z","shell.execute_reply":"2021-03-19T01:22:50.729247Z"},"id":"e6GM-3evH1Sz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904989,"user_tz":-480,"elapsed":11159,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"39a1f02f-f1dc-4e33-d88d-86823df729ea"},"source":["# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n","grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n","\n","print('dy/dx0:', grad['x0'])\n","print('dy/dx1:', grad['x1'].numpy())"],"execution_count":15,"outputs":[{"output_type":"stream","text":["dy/dx0: None\n","dy/dx1: 0.9999546\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2g1nKB6P-OnA"},"source":["## Intermediate results\n","\n","You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.734771Z","iopub.status.busy":"2021-03-19T01:22:50.733754Z","iopub.status.idle":"2021-03-19T01:22:50.737344Z","shell.execute_reply":"2021-03-19T01:22:50.736899Z"},"id":"7XaPRAwUyYms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904990,"user_tz":-480,"elapsed":11141,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"1e7ce632-3293-4988-bda8-fafb431d084f"},"source":["x = tf.constant(3.0)\n","\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x * x\n","  z = y * y\n","\n","# Use the tape to compute the gradient of z with respect to the\n","# intermediate value y.\n","# dz_dx = 2 * y, where y = x ** 2 (= 9)\n","print(tape.gradient(z, y).numpy())"],"execution_count":16,"outputs":[{"output_type":"stream","text":["18.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ISkXuY7YzIcS"},"source":["By default, the resources held by a `GradientTape` are released as soon as the `GradientTape.gradient` method is called. To compute multiple gradients over the same computation, create a gradient tape with `persistent=True`. This allows multiple calls to the `gradient` method as resources are released when the tape object is garbage collected. For example:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.741719Z","iopub.status.busy":"2021-03-19T01:22:50.741071Z","iopub.status.idle":"2021-03-19T01:22:50.745659Z","shell.execute_reply":"2021-03-19T01:22:50.746022Z"},"id":"zZaCm3-9zVCi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904992,"user_tz":-480,"elapsed":11127,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"5f5c69e0-b14c-408f-ef2e-6837cc9163b8"},"source":["x = tf.constant(3.0)\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  tape.watch(x)\n","  y = x * x\n","  z = y * y\n","\n","print(tape.gradient(z, y).numpy())  # 18.0 (2 * x**2)\n","print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["18.0\n","6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.749607Z","iopub.status.busy":"2021-03-19T01:22:50.748988Z","iopub.status.idle":"2021-03-19T01:22:50.751260Z","shell.execute_reply":"2021-03-19T01:22:50.751680Z"},"id":"j8bv_jQFg6CN","executionInfo":{"status":"ok","timestamp":1616487904993,"user_tz":-480,"elapsed":11114,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["del tape   # Drop the reference to the tape"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_ZY-9BUB7vX"},"source":["## Notes on performance\n","\n","* There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n","\n","* Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n","\n","  For efficiency, some ops (like `ReLU`) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use `persistent=True` on your tape, *nothing is discarded* and your peak memory usage will be higher."]},{"cell_type":"markdown","metadata":{"id":"9dLBpZsJebFq"},"source":["## Gradients of non-scalar targets"]},{"cell_type":"markdown","metadata":{"id":"7pldU9F5duP2"},"source":["A gradient is fundamentally an operation on a scalar."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.756995Z","iopub.status.busy":"2021-03-19T01:22:50.756351Z","iopub.status.idle":"2021-03-19T01:22:50.761446Z","shell.execute_reply":"2021-03-19T01:22:50.760975Z"},"id":"qI0sDV_WeXBb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904994,"user_tz":-480,"elapsed":11098,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"7da161bc-5c64-40bf-f9d5-ba73d49af470"},"source":["x = tf.Variable(2.0)\n","with tf.GradientTape(persistent=True) as tape:\n","  y0 = x**2\n","  y1 = 1 / x\n","\n","print(tape.gradient(y0, x).numpy())\n","print(tape.gradient(y1, x).numpy())"],"execution_count":19,"outputs":[{"output_type":"stream","text":["4.0\n","-0.25\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"COEyYp34fxj4"},"source":["Thus, if you ask for the gradient of multiple targets, the result for each source is:\n","\n","* The gradient of the sum of the targets, or equivalently\n","* The sum of the gradients of each target."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.765972Z","iopub.status.busy":"2021-03-19T01:22:50.765297Z","iopub.status.idle":"2021-03-19T01:22:50.769183Z","shell.execute_reply":"2021-03-19T01:22:50.769576Z"},"id":"o4a6_YOcfWKS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904995,"user_tz":-480,"elapsed":11083,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"86706ab8-b383-49f5-c122-e916cfbbb9b5"},"source":["x = tf.Variable(2.0)\n","with tf.GradientTape(persistent=True) as tape:\n","  y0 = x**2\n","  y1 = 1 / x\n","\n","print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy()) # dict\n","print(tape.gradient([y0, y1], x).numpy()) # list"],"execution_count":20,"outputs":[{"output_type":"stream","text":["3.75\n","3.75\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2nsO4pcI23l","executionInfo":{"status":"ok","timestamp":1616487904996,"user_tz":-480,"elapsed":11068,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"3fd3b47b-3ff6-46f6-b819-d80b5ccfd562"},"source":["# For multi-task learning, define final_loss within tf.GradientTape()\n","x = tf.Variable(2.0)\n","with tf.GradientTape() as tape:\n","    y0 = x**2\n","    y1 = 1/x\n","    alpha = tf.constant(0.3)\n","    final_loss = alpha * y0 + (1 - alpha) * y1\n","\n","print(tape.gradient(final_loss, x).numpy())"],"execution_count":21,"outputs":[{"output_type":"stream","text":["1.0250001\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uvP-mkBMgbym"},"source":["Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.774011Z","iopub.status.busy":"2021-03-19T01:22:50.773407Z","iopub.status.idle":"2021-03-19T01:22:50.777405Z","shell.execute_reply":"2021-03-19T01:22:50.777803Z"},"id":"DArPWqsSh5un","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487904997,"user_tz":-480,"elapsed":11055,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"758476f6-0440-4fa5-e8b5-f1087bb290a6"},"source":["x = tf.Variable(2.)\n","\n","with tf.GradientTape() as tape:\n","  y = x * [3., 4.]\n","\n","print(tape.gradient(y, x).numpy())"],"execution_count":22,"outputs":[{"output_type":"stream","text":["7.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"flDbx68Zh5Lb"},"source":["This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n","\n","If you need a separate gradient for each item, refer to [Jacobians](advanced_autodiff.ipynb#jacobians)."]},{"cell_type":"markdown","metadata":{"id":"iwFswok8RAly"},"source":["In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.782532Z","iopub.status.busy":"2021-03-19T01:22:50.781949Z","iopub.status.idle":"2021-03-19T01:22:50.790536Z","shell.execute_reply":"2021-03-19T01:22:50.790983Z"},"id":"JQvk_jnMmTDS","executionInfo":{"status":"ok","timestamp":1616487905858,"user_tz":-480,"elapsed":11913,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["x = tf.linspace(-10.0, 10.0, 200+1)\n","\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = tf.nn.sigmoid(x)\n","\n","dy_dx = tape.gradient(y, x)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:50.810395Z","iopub.status.busy":"2021-03-19T01:22:50.803457Z","iopub.status.idle":"2021-03-19T01:22:51.055330Z","shell.execute_reply":"2021-03-19T01:22:51.055955Z"},"id":"e_f2QgDPmcPE","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1616487905876,"user_tz":-480,"elapsed":11915,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"2097f465-c4e4-4b01-e17f-e620527a01ee"},"source":["plt.plot(x, y, label='y')\n","plt.plot(x, dy_dx, label='dy/dx')\n","plt.legend()\n","_ = plt.xlabel('x')"],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"6kADybtQzYj4"},"source":["## Control flow\n","\n","Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, `if` and `while` statements).\n","\n","Here a different variable is used on each branch of an `if`. The gradient only connects to the variable that was used:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.062698Z","iopub.status.busy":"2021-03-19T01:22:51.062006Z","iopub.status.idle":"2021-03-19T01:22:51.066812Z","shell.execute_reply":"2021-03-19T01:22:51.066299Z"},"id":"ciFLizhrrjy7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905877,"user_tz":-480,"elapsed":11894,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"529116d5-a45e-408f-a8d2-ac0a1ad64851"},"source":["x = tf.constant(1.0)\n","\n","v0 = tf.Variable(2.0)\n","v1 = tf.Variable(2.0)\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  tape.watch(x) # Just for testing in the next block\n","  if x > 0.0:\n","    result = v0\n","  else:\n","    result = v1**2 \n","\n","dv0, dv1 = tape.gradient(result, [v0, v1])\n","\n","print(dv0)\n","print(dv1)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["tf.Tensor(1.0, shape=(), dtype=float32)\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HKnLaiapsjeP"},"source":["Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n","\n","Depending on the value of `x` in the above example, the tape either records `result = v0` or `result = v1**2`. The gradient with respect to `x` is always `None`."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.071362Z","iopub.status.busy":"2021-03-19T01:22:51.070643Z","iopub.status.idle":"2021-03-19T01:22:51.072987Z","shell.execute_reply":"2021-03-19T01:22:51.073393Z"},"id":"8k05WmuAwPm7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905879,"user_tz":-480,"elapsed":11875,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"cff91e80-af89-41ea-b55f-d3fcefff5d70"},"source":["dx = tape.gradient(result, x)\n","\n","print(dx)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"egypBxISAHhx"},"source":["## Getting a gradient of `None`\n","\n","When a target is not connected to a source you will get a gradient of `None`.\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.077800Z","iopub.status.busy":"2021-03-19T01:22:51.077181Z","iopub.status.idle":"2021-03-19T01:22:51.081328Z","shell.execute_reply":"2021-03-19T01:22:51.080841Z"},"id":"CU185WDM81Ut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905881,"user_tz":-480,"elapsed":11858,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"392488f9-21a3-4c99-b0bd-3274bf667fd6"},"source":["x = tf.Variable(2.)\n","y = tf.Variable(3.)\n","\n","with tf.GradientTape() as tape:\n","  z = y * y\n","print(tape.gradient(z, x))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sZbKpHfBRJym"},"source":["Here `z` is obviously not connected to `x`, but there are several less-obvious ways that a gradient can be disconnected."]},{"cell_type":"markdown","metadata":{"id":"eHDzDOiQ8xmw"},"source":["### 1. Replaced a variable with a tensor\n","\n","In the section on [\"controlling what the tape watches\"](#watches) you saw that the tape will automatically watch a `tf.Variable` but not a `tf.Tensor`.\n","\n","One common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`. Here is an example:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.086395Z","iopub.status.busy":"2021-03-19T01:22:51.085557Z","iopub.status.idle":"2021-03-19T01:22:51.090278Z","shell.execute_reply":"2021-03-19T01:22:51.089801Z"},"id":"QPKY4Tn9zX7_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905882,"user_tz":-480,"elapsed":11840,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"28aa15a7-8581-42ba-f88a-182c11cbaba9"},"source":["x = tf.Variable(2.0)\n","\n","for epoch in range(2):\n","  with tf.GradientTape() as tape:\n","    y = x+1\n","\n","  print(type(x).__name__, \":\", tape.gradient(y, x))\n","  x = x + 1   # This should be `x.assign_add(1)`"],"execution_count":28,"outputs":[{"output_type":"stream","text":["ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n","EagerTensor : None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3gwZKxgA97an"},"source":["### 2. Did calculations outside of TensorFlow\n","\n","The tape can't record the gradient path if the calculation exits TensorFlow.\n","For example:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.095376Z","iopub.status.busy":"2021-03-19T01:22:51.094577Z","iopub.status.idle":"2021-03-19T01:22:51.098167Z","shell.execute_reply":"2021-03-19T01:22:51.098504Z"},"id":"jmoLCDJb_yw1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905883,"user_tz":-480,"elapsed":11825,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"5d28fd1a-2e4c-4066-c9f5-a46933be5921"},"source":["x = tf.Variable([[1.0, 2.0],\n","                 [3.0, 4.0]], dtype=tf.float32)\n","\n","with tf.GradientTape() as tape:\n","  x2 = x**2\n","\n","  # This step is calculated with NumPy\n","  y = np.mean(x2, axis=0)\n","\n","  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n","  # using `tf.convert_to_tensor`.\n","  y = tf.reduce_mean(y, axis=0)\n","\n","print(tape.gradient(y, x))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p3YVfP3R-tp7"},"source":["### 3. Took gradients through an integer or string\n","\n","Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n","\n","Nobody expects strings to be differentiable, but it's easy to accidentally create an `int` constant or variable if you don't specify the `dtype`."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.103946Z","iopub.status.busy":"2021-03-19T01:22:51.103117Z","iopub.status.idle":"2021-03-19T01:22:51.108993Z","shell.execute_reply":"2021-03-19T01:22:51.108487Z"},"id":"9jlHXHqfASU3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905884,"user_tz":-480,"elapsed":11813,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"71ace7d6-5c34-426e-d347-ac936a76413e"},"source":["x = tf.constant(10) # Change to 10.0\n","\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x * x\n","\n","print(tape.gradient(y, x))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n","WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n","WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RsdP_mTHX9L1"},"source":["TensorFlow doesn't automatically cast between types, so, in practice, you'll often get a type error instead of a missing gradient."]},{"cell_type":"markdown","metadata":{"id":"WyAZ7C8qCEs6"},"source":["### 4. Took gradients through a stateful object\n","\n","State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n","\n","A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a _value_, but no _state_. All the operations discussed so far are also stateless: the output of a `tf.matmul` only depends on its inputs.\n","\n","A `tf.Variable` has internal state—its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.113936Z","iopub.status.busy":"2021-03-19T01:22:51.113311Z","iopub.status.idle":"2021-03-19T01:22:51.117358Z","shell.execute_reply":"2021-03-19T01:22:51.117693Z"},"id":"C1tLeeRFE479","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905885,"user_tz":-480,"elapsed":11797,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f2b3dfeb-1d2e-4be7-f0d5-55875b8b33d9"},"source":["x0 = tf.Variable(3.0)\n","x1 = tf.Variable(0.0)\n","\n","with tf.GradientTape() as tape:\n","  # Update x1 = x1 + x0.\n","  x1.assign_add(x0)\n","  # The tape starts recording from x1.\n","  y = x1**2   # y = (x1 + x0)**2\n","\n","# This doesn't work.\n","print(tape.gradient(y, x0))   # dy/dx0 = 2*(x1 + x0)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xKA92-dqF2r-"},"source":["Similarly, `tf.data.Dataset` iterators and `tf.queue`s are stateful, and will stop all gradients on tensors that pass through them."]},{"cell_type":"markdown","metadata":{"id":"HHvcDGIbOj2I"},"source":["## No gradient registered"]},{"cell_type":"markdown","metadata":{"id":"aoc-A6AxVqry"},"source":["Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n","\n","The `tf.raw_ops` page shows which low-level ops have gradients registered.\n","\n","If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning `None`. This way you know something has gone wrong.\n","\n","For example, the `tf.image.adjust_contrast` function wraps `raw_ops.AdjustContrastv2`, which could have a gradient but the gradient is not implemented:\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.122994Z","iopub.status.busy":"2021-03-19T01:22:51.122410Z","iopub.status.idle":"2021-03-19T01:22:51.126621Z","shell.execute_reply":"2021-03-19T01:22:51.127028Z"},"id":"HSb20FXc_V0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905886,"user_tz":-480,"elapsed":11782,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"a195c8ed-f041-4dae-866c-1ba1c3c17aa8"},"source":["image = tf.Variable([[[0.5, 0.0, 0.0]]])\n","delta = tf.Variable(0.1)\n","\n","with tf.GradientTape() as tape:\n","  new_image = tf.image.adjust_contrast(image, delta)\n","\n","try:\n","  print(tape.gradient(new_image, [image, delta]))\n","  assert False   # This should not happen.\n","except LookupError as e:\n","  print(f'{type(e).__name__}: {e}')\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["LookupError: gradient registry has no entry for: AdjustContrastv2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pDoutjzATiEm"},"source":["If you need to differentiate through this op, you'll either need to implement the gradient and register it (using `tf.RegisterGradient`) or re-implement the function using other ops."]},{"cell_type":"markdown","metadata":{"id":"GCTwc_dQXp2W"},"source":["## Zeros instead of None"]},{"cell_type":"markdown","metadata":{"id":"TYDrVogA89eA"},"source":["In some cases it would be convenient to get 0 instead of `None` for unconnected gradients.  You can decide what to return when you have unconnected gradients using the `unconnected_gradients` argument:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-03-19T01:22:51.131767Z","iopub.status.busy":"2021-03-19T01:22:51.131053Z","iopub.status.idle":"2021-03-19T01:22:51.135770Z","shell.execute_reply":"2021-03-19T01:22:51.135265Z"},"id":"U6zxk1sf9Ixx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616487905888,"user_tz":-480,"elapsed":11770,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"22a287f0-06c9-4f95-9849-fefb29a43d8d"},"source":["x = tf.Variable([2., 2.])\n","y = tf.Variable(3.)\n","\n","with tf.GradientTape() as tape:\n","  z = y**2\n","print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"],"name":"stdout"}]}]}
>>>>>>> Stashed changes
